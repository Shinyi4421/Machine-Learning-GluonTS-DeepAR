{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054136c7-c443-4018-8150-ea217a802744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This project uses GluonTS, a Python toolkit for probabilistic time series modeling.\n",
    "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "# You may not use this file except in compliance with the License.\n",
    "# A copy of the License is located at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# or in the \"license\" file accompanying this file. This file is distributed \n",
    "# on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, \n",
    "# either express or implied. See the License for the specific language \n",
    "# governing permissions and limitations under the License.\n",
    "\n",
    "# This file uses the GluonTS library for probabilistic time series modeling.\n",
    "# See https://github.com/awslabs/gluonts for more information.\n",
    "\n",
    "import mariadb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Set up database connection parameters\n",
    "config = {\n",
    "    'host': '#yuor host',\n",
    "    'user': '#yuor user',\n",
    "    'password': '#yuor password',\n",
    "    'port': #yuor port,\n",
    "    'database': 'deepar_train'\n",
    "}\n",
    "\n",
    "# Establish database connection\n",
    "try:\n",
    "    conn = mariadb.connect(**config)\n",
    "    print(\"Connection successful\")\n",
    "except mariadb.Error as e:\n",
    "    print(f\"Connection failed: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the table name to be extracted\n",
    "tables = ['deepar_validation_dataframe']\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "# Extract data from each table and store it in a dictionary\n",
    "for table in tables:\n",
    "    query = f\"SELECT * FROM {table}\"\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    column_names = [i[0] for i in cursor.description]\n",
    "    dataframes[table] = pd.DataFrame(result, columns=column_names)\n",
    "\n",
    "# Close cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Process data from each table\n",
    "for table_name, df in dataframes.items():\n",
    "    # Convert Date column to datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Generate item_id\n",
    "    df['item_id'] = df['Area']\n",
    "\n",
    "    # Generate start date based on Area\n",
    "    start_dates = df.groupby('Area')['Date'].min().reset_index()\n",
    "    df = df.merge(start_dates, on='Area', suffixes=('', '_start'))\n",
    "    df = df.rename(columns={'Date_start': 'start'})\n",
    "\n",
    "    # Generate target and observed_values\n",
    "    df = df.sort_values(by=['item_id', 'Date'])\n",
    "    df['observed_values'] = 1\n",
    "    df = df.set_index(['item_id', 'Date'])\n",
    "\n",
    "    # Fill missing values\n",
    "    df['VALUE'] = df['VALUE'].fillna(0)\n",
    "    df['observed_values'] = np.where(df['observed_values'] == 0, 0, 1)\n",
    "    df['month'] = df['month'].fillna(0)\n",
    "    df['month_sin'] = df['month_sin'].fillna(0)\n",
    "    df['month_cos'] = df['month_cos'].fillna(0)\n",
    "    df['Combined_Recovery_Rate'] = df['Combined_Recovery_Rate'].fillna(0)\n",
    "    df['cumulative_trend_part'] = df['cumulative_trend_part'].fillna(0)\n",
    "    df['sales_gradient'] = df['sales_gradient'].fillna(0)\n",
    "    df['total_fossil'] = df['total_fossil'].fillna(0)\n",
    "    df['total_sales'] = df['total_sales'].fillna(0)\n",
    "    df['Combined_Transportation'] = df['Combined_Transportation'].fillna(0)\n",
    "    df['Iron_Steel_Products'] = df['Iron_Steel_Products'].fillna(0)\n",
    "\n",
    "    # Handle missing values in dynamic features\n",
    "    df['month'] = df['month'].fillna(-1)\n",
    "    df['month_sin'] = df['month_sin'].fillna(-1)\n",
    "    df['month_cos'] = df['month_cos'].fillna(-1)\n",
    "    df['Combined_Recovery_Rate'] = df['Combined_Recovery_Rate'].fillna(-1)\n",
    "    df['cumulative_trend_part'] = df['cumulative_trend_part'].fillna(-1)\n",
    "    df['sales_gradient'] = df['sales_gradient'].fillna(-1)\n",
    "    df['total_fossil'] = df['total_fossil'].fillna(-1)\n",
    "    df['total_sales'] = df['total_sales'].fillna(-1)\n",
    "    df['Combined_Transportation'] = df['Combined_Transportation'].fillna(-1)\n",
    "    df['Iron_Steel_Products'] = df['Iron_Steel_Products'].fillna(-1)\n",
    "\n",
    "    # Define static features\n",
    "    feat_static_cat = [1, 2]  # 1 represents United States of America, 2 represents mtCO2\n",
    "\n",
    "    # Find the earliest start date\n",
    "    earliest_start = df['start'].min()\n",
    "\n",
    "    # Split the table and save\n",
    "    list_data = []\n",
    "    for item_id, group in df.groupby('item_id'):\n",
    "        target = group['VALUE'].values.tolist()\n",
    "        observed_values = group['observed_values'].values.tolist()\n",
    "        start = pd.Timestamp(group['start'].iloc[0])\n",
    "        fill_length = (start.year - earliest_start.year) * 12 + (start.month - earliest_start.month)\n",
    "        month = group['month'].values.tolist()\n",
    "        month_sin = group['month_sin'].values.tolist()\n",
    "        month_cos = group['month_cos'].values.tolist()\n",
    "        Combined_Recovery_Rate = group['Combined_Recovery_Rate'].values.tolist()\n",
    "        cumulative_trend_part = group['cumulative_trend_part'].values.tolist()\n",
    "        sales_gradient = group['sales_gradient'].values.tolist()\n",
    "        total_fossil = group['total_fossil'].values.tolist()\n",
    "        total_sales = group['total_sales'].values.tolist()\n",
    "        Combined_Transportation = group['Combined_Transportation'].values.tolist()\n",
    "        Iron_Steel_Products = group['Iron_Steel_Products'].values.tolist()\n",
    "\n",
    "        # Forward fill with -1\n",
    "        target = [-1] * fill_length + target\n",
    "        observed_values = [0] * fill_length + observed_values\n",
    "        month = [-1] * fill_length + month\n",
    "        month_sin = [-1] * fill_length + month_sin\n",
    "        month_cos = [-1] * fill_length + month_cos\n",
    "        Combined_Recovery_Rate = [-1] * fill_length + Combined_Recovery_Rate\n",
    "        cumulative_trend_part = [-1] * fill_length + cumulative_trend_part\n",
    "        sales_gradient = [-1] * fill_length + sales_gradient\n",
    "        total_fossil = [-1] * fill_length + total_fossil\n",
    "        total_sales = [-1] * fill_length + total_sales\n",
    "        Combined_Transportation = [-1] * fill_length + Combined_Transportation\n",
    "        Iron_Steel_Products = [-1] * fill_length + Iron_Steel_Products\n",
    "        \n",
    "        feat_dynamic_real = [\n",
    "            month,\n",
    "            month_sin,\n",
    "            month_cos,\n",
    "            Combined_Recovery_Rate,\n",
    "            cumulative_trend_part,\n",
    "            sales_gradient,\n",
    "            total_fossil,\n",
    "            total_sales,\n",
    "            Combined_Transportation,\n",
    "            Iron_Steel_Products\n",
    "        ]\n",
    "\n",
    "        list_data.append({\n",
    "            \"item_id\": item_id,\n",
    "            \"start\": earliest_start.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"target\": target,\n",
    "            \"feat_static_cat\": feat_static_cat,  # Fixed United States of America as 1\n",
    "            \"feat_dynamic_real\": feat_dynamic_real  # Merged into a two-dimensional list\n",
    "        })\n",
    "\n",
    "    train_data = ListDataset(list_data, freq=\"M\")\n",
    "    variable_filename = table_name.replace(' ', '_')\n",
    "    output_path = f'F:/DeepAR_models/Data_clean/gluonts_listDataset_{variable_filename}.pkl'\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    print(f\"Dataset generated and saved as {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
